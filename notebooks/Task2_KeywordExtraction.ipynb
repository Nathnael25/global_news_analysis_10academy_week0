{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Keyword Extraction and Topic Modeling\n",
    "\n",
    "## Step 1: Perform Keyword Extraction\n",
    "\n",
    "In this section, we will perform keyword extraction using the TF-IDF (Term Frequency-Inverse Document Frequency) method. The goal is to identify the most significant words in the headlines and bodies of the news articles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def extract_keywords(texts):\n",
    "    vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "    tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    return feature_names\n",
    "\n",
    "# Example usage\n",
    "texts = [\"This is a sample news article.\", \"Keyword extraction from text.\"]\n",
    "keywords = extract_keywords(texts)\n",
    "print(keywords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Perform Topic Modeling\n",
    "\n",
    "Next, we will perform topic modeling using Latent Dirichlet Allocation (LDA). This technique will help us identify underlying topics in the news articles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "def perform_topic_modeling(texts, n_topics=5):\n",
    "    vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "    tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "    lda = LatentDirichletAllocation(n_components=n_topics, random_state=0)\n",
    "    lda.fit(tfidf_matrix)\n",
    "    return lda, vectorizer\n",
    "\n",
    "# Example usage\n",
    "lda, vectorizer = perform_topic_modeling(texts)\n",
    "print(lda.components_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of Topics\n",
    "\n",
    "To better understand the topics identified, we will visualize the topics in a 2D space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def visualize_topics(lda, tfidf_matrix):\n",
    "    tsne = TSNE(n_components=2, random_state=0)\n",
    "    topic_weights = lda.transform(tfidf_matrix)\n",
    "    tsne_results = tsne.fit_transform(topic_weights)\n",
    "    \n",
    "    plt.scatter(tsne_results[:, 0], tsne_results[:, 1], c=topic_weights.argmax(axis=1))\n",
    "    plt.title('Topic Visualization using t-SNE')\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "visualize_topics(lda, tfidf_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Event Modeling\n",
    "\n",
    "## Step 1: Cluster News Articles by Events\n",
    "\n",
    "In this section, we will cluster news articles based on their content to identify different events. We will use the K-Means clustering algorithm for this task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def cluster_news_articles(tfidf_matrix, n_clusters=10):\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "    clusters = kmeans.fit_predict(tfidf_matrix)\n",
    "    return clusters\n",
    "\n",
    "# Example usage in a notebook or script\n",
    "clusters = cluster_news_articles(tfidf_matrix)\n",
    "print(clusters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Analyze Event Reporting\n",
    "\n",
    "Now that we have clustered the articles, let's analyze which news sites report events the earliest and which events have the highest reporting frequency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example analysis code (adjust based on your data)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def analyze_event_reporting(clusters, articles):\n",
    "    articles['cluster'] = clusters\n",
    "    earliest_reporting = articles.groupby('cluster')['published_at'].min()\n",
    "    highest_reporting = articles['cluster'].value_counts()\n",
    "    \n",
    "    print(\"Earliest reporting by cluster:\")\n",
    "    print(earliest_reporting)\n",
    "    print(\"\\nHighest reporting by cluster:\")\n",
    "    print(highest_reporting)\n",
    "\n",
    "# Assuming you have a dataframe `articles` with a 'published_at' column\n",
    "# analyze_event_reporting(clusters, articles)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: ML Model Versioning\n",
    "\n",
    "## Step 1: Version ML Models\n",
    "\n",
    "For this task, we will version our ML models using MLFlow to ensure that we can track different versions of our models, along with their associated data and code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "def save_model(model, model_name):\n",
    "    mlflow.sklearn.log_model(model, model_name)\n",
    "    mlflow.register_model(model_uri=model_name, name=model_name)\n",
    "\n",
    "# Example usage\n",
    "save_model(lda, \"LDA_Topic_Model\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
